{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Emotion Detection Unsing OpenCV and Keras\nIn these project we will use keras:the Python deep learning api for emotion detection using live camera of your system.\n### Importing required modules","metadata":{}},{"cell_type":"code","source":"import keras \nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Dropout,Activation,Flatten,BatchNormalization\nfrom keras.layers import Conv2D,MaxPooling2D\nimport os","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-11-28T12:07:38.104383Z","iopub.execute_input":"2022-11-28T12:07:38.104918Z","iopub.status.idle":"2022-11-28T12:07:44.410715Z","shell.execute_reply.started":"2022-11-28T12:07:38.104817Z","shell.execute_reply":"2022-11-28T12:07:44.409807Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"### Defining constant values\n\n* num_classses : The number of classes or the emotions that will be dealing with in the training our model                \n* img_rows,img_cols : These number define the size of the image array that we will be feeding the neural network        \n* batch_size :  This variable defines the batch size.The batch size is a number of samples processed before the model is updated. The number of epochs is the number of complete passes through the training dataset. The size of a batch must be more than or equal to one and less than or equal to the number of samples in the training dataset.","metadata":{}},{"cell_type":"code","source":"num_classes=7\nimg_rows,img_cols=48,48\nbatch_size=32","metadata":{"execution":{"iopub.status.busy":"2022-11-28T12:07:44.412549Z","iopub.execute_input":"2022-11-28T12:07:44.413434Z","iopub.status.idle":"2022-11-28T12:07:44.417643Z","shell.execute_reply.started":"2022-11-28T12:07:44.413388Z","shell.execute_reply":"2022-11-28T12:07:44.416697Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"### Upload the data direction","metadata":{}},{"cell_type":"code","source":"train_data_dir=\"/kaggle/input/emotion-detection-fer/train\"\ntest_data_dir=\"/kaggle/input/emotion-detection-fer/test\"","metadata":{"execution":{"iopub.status.busy":"2022-11-28T12:07:44.419527Z","iopub.execute_input":"2022-11-28T12:07:44.420098Z","iopub.status.idle":"2022-11-28T12:07:44.431526Z","shell.execute_reply.started":"2022-11-28T12:07:44.420037Z","shell.execute_reply":"2022-11-28T12:07:44.430183Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"### Image augmentation\nImage Data Augmentation is a technique that can be used to artificially expand the size of a training dataset by creating modified versions of images in the dataset.","metadata":{}},{"cell_type":"markdown","source":"The train_datagen variable will artificially expand the dataset using the following:\n\n* rotation_range: Degree range for random rotations. Here i am using 30 degrees.\n* shear_range: Shear Intensity (Shear angle in counter-clockwise direction in degrees). Here i am using 0.3 as shear range.\n* zoom_range: Range for random zoom.Here i am using 0.3 as zoom range.\n* width_shift_range: This shifts the images by a value across its width.\n* height_shift_range : This shifts the images by a value across its height.\n* horizontal_flip: This flips the images horizontally.\n* fill_mode: This is used to fill in the pixels after making changes to the orientation of the images by the above used methods. Here i am using ‘nearest’ as the fill mode as i am instructing it to fill the missing pixels in the image with the nearby pixels.\nHere I am just resclaing the validation data and not performing any other augmentaions as i want to check the model with raw data that is different from the data used in the training of the model.","metadata":{}},{"cell_type":"code","source":"train_datagen=ImageDataGenerator(\n    rescale=1./255,\n    rotation_range=30,\n    shear_range=0.3,\n    zoom_range=0.3,\n    width_shift_range=0.4,\n    height_shift_range=0.4,\n    horizontal_flip=True,\n    fill_mode='nearest'\n    )\ntest_datagen=ImageDataGenerator(rescale=1./255)","metadata":{"execution":{"iopub.status.busy":"2022-11-28T12:07:44.434307Z","iopub.execute_input":"2022-11-28T12:07:44.434707Z","iopub.status.idle":"2022-11-28T12:07:44.443595Z","shell.execute_reply.started":"2022-11-28T12:07:44.434673Z","shell.execute_reply":"2022-11-28T12:07:44.442649Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":" train_generator = train_datagen.flow_from_directory(\n train_data_dir, color_mode='grayscale', target_size=(img_rows,img_cols),\n batch_size=32, class_mode='categorical', shuffle=True)\n test_generator = test_datagen.flow_from_directory(\n test_data_dir, color_mode='grayscale', target_size=(img_rows,img_cols),\n batch_size=batch_size, class_mode='categorical',shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2022-11-28T12:07:44.446060Z","iopub.execute_input":"2022-11-28T12:07:44.448072Z","iopub.status.idle":"2022-11-28T12:07:57.019621Z","shell.execute_reply.started":"2022-11-28T12:07:44.448026Z","shell.execute_reply":"2022-11-28T12:07:57.018094Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Found 28709 images belonging to 7 classes.\nFound 7178 images belonging to 7 classes.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The options given are:\n\n* directory: The directory of the dataset.\n* color_mode: Here i am converting the images to gray-scale as i am not interested in the color of the images but only the expressions.\n* target_size: Convert the images to a uniform size.\n* batch_size: To make baches of data to train.\n* class_mode: Here i am using ‘categorical’ as the class mode as i am categorizing my images into 5 classes.\n* shuffle: To shuffle the dataset for better training.","metadata":{}},{"cell_type":"markdown","source":"### Build the model\nHere we are using a Sequential model which defines that all the layers in the network will be one after the other sequentially and storing it in a variable model.","metadata":{}},{"cell_type":"code","source":"model=Sequential()","metadata":{"execution":{"iopub.status.busy":"2022-11-28T12:07:57.021249Z","iopub.execute_input":"2022-11-28T12:07:57.021882Z","iopub.status.idle":"2022-11-28T12:07:57.082768Z","shell.execute_reply.started":"2022-11-28T12:07:57.021848Z","shell.execute_reply":"2022-11-28T12:07:57.081833Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"2022-11-28 12:07:57.059982: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The network consists of 7 blocks","metadata":{}},{"cell_type":"markdown","source":"Before compiling we will create 3 things using **keras.callbacks**: \n**Block-1 layers in the order of occurrence are as follows :\n* Conv2D layer- This layer creates a convolutional layer for the network. Here i am creating a layer with 32 filters and a filter size of (3,3) with padding=’same’ to pad the image and using the kernel initializer he_normal. I have added 2 convolutional layers each followed by an activation and batch normalization layers.\n* Activation layer — I am using a elu activation.\n* BatchNormalization — Normalize the activations of the previous layer at each batch, i.e. applies a transformation that maintains the mean activation close to 0 and the activation standard deviation close to 1.\n* MaxPooling2D layer — Downsamples the input representation by taking the maximum value over the window defined by pool_size for each dimension along the features axis.Here i have used the pool_size as (2,2).\n* Dropout: Dropout is a technique where randomly selected neurons are ignored during training. Here i am using dropout as 0.5 which means that it will ignore half of the neurons.\n**Block-2 layers in the order of occurrence are as follows :\n* Same layers as block-1 but the convolutional layers have 64 filters.\n**Block-3 layers in the order of occurrence are as follows :\n* Same layers as block-1 but the convolutional layers have 128 filters.\n**Block-4 layers in the order of occurrence are as follows :\n* Same layers as block-1 but the convolutional layers have 256 filters.\n**Block-5 layers in the order of occurrence are as follows :\n* Flatten layer — To flatten the output of the previous layers in a flat layer or in other words in the form of a vector.\n* Dense layer — A densely connected layer where each neuron is connected to every other neuron. Here i am using 64 units or 64 neurons with a kernel initializer — he_normal.\n* These layers are followed by activation layer with elu activation , batch normalization and finally a dropout with 50% dropout.\n**Block-6 layers in the order of occurrence are as follows :\n* Same layers as block 5 but without flatten layer as the input for this block is already flattened.\n**Block-7 layers in the order of occurrence are as follows :\n* Dense layer — Finally in the final block of the network i am using num_classes to create a dense layer having units=number of classes with a he_normal initializer.\n* Activation layer — Here i am using a softmax layer which is used for multi-class classifications.","metadata":{}},{"cell_type":"code","source":"#first block\nmodel.add(Conv2D(32,(3,3),padding='same',kernel_initializer='he_normal',\n                 input_shape=(img_rows,img_cols,1)))\nmodel.add(Activation('elu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(32,(3,3),padding='same',kernel_initializer='he_normal',\n                 input_shape=(img_rows,img_cols,1)))\nmodel.add(Activation('elu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Dropout(0.2))\n#Block-2\nmodel.add(Conv2D(64,(3,3),padding='same',kernel_initializer='he_normal'))\nmodel.add(Activation('elu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(64,(3,3),padding='same',kernel_initializer='he_normal'))\nmodel.add(Activation('elu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Dropout(0.2))\n#Block-3\nmodel.add(Conv2D(128,(3,3),padding='same',kernel_initializer='he_normal'))\nmodel.add(Activation('elu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(128,(3,3),padding='same',kernel_initializer='he_normal'))\nmodel.add(Activation('elu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Dropout(0.2))\n#Block-4\nmodel.add(Conv2D(256,(3,3),padding='same',kernel_initializer='he_normal'))\nmodel.add(Activation('elu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(256,(3,3),padding='same',kernel_initializer='he_normal'))\nmodel.add(Activation('elu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Dropout(0.2))\n#Block-5\nmodel.add(Flatten())\nmodel.add(Dense(64,kernel_initializer='he_normal'))\nmodel.add(Activation('elu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.5))\n#Block-6\nmodel.add(Dense(64,kernel_initializer='he_normal'))\nmodel.add(Activation('elu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.5))\n#Block-7\nmodel.add(Dense(num_classes,kernel_initializer='he_normal'))\nmodel.add(Activation('softmax'))","metadata":{"execution":{"iopub.status.busy":"2022-11-28T12:07:57.084476Z","iopub.execute_input":"2022-11-28T12:07:57.085216Z","iopub.status.idle":"2022-11-28T12:07:57.650329Z","shell.execute_reply.started":"2022-11-28T12:07:57.085171Z","shell.execute_reply":"2022-11-28T12:07:57.649296Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"### the overall structure of the model:","metadata":{}},{"cell_type":"code","source":"print(model.summary())","metadata":{"execution":{"iopub.status.busy":"2022-11-28T12:07:57.651718Z","iopub.execute_input":"2022-11-28T12:07:57.652175Z","iopub.status.idle":"2022-11-28T12:07:57.664062Z","shell.execute_reply.started":"2022-11-28T12:07:57.652132Z","shell.execute_reply":"2022-11-28T12:07:57.662815Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d (Conv2D)              (None, 48, 48, 32)        320       \n_________________________________________________________________\nactivation (Activation)      (None, 48, 48, 32)        0         \n_________________________________________________________________\nbatch_normalization (BatchNo (None, 48, 48, 32)        128       \n_________________________________________________________________\nconv2d_1 (Conv2D)            (None, 48, 48, 32)        9248      \n_________________________________________________________________\nactivation_1 (Activation)    (None, 48, 48, 32)        0         \n_________________________________________________________________\nbatch_normalization_1 (Batch (None, 48, 48, 32)        128       \n_________________________________________________________________\nmax_pooling2d (MaxPooling2D) (None, 24, 24, 32)        0         \n_________________________________________________________________\ndropout (Dropout)            (None, 24, 24, 32)        0         \n_________________________________________________________________\nconv2d_2 (Conv2D)            (None, 24, 24, 64)        18496     \n_________________________________________________________________\nactivation_2 (Activation)    (None, 24, 24, 64)        0         \n_________________________________________________________________\nbatch_normalization_2 (Batch (None, 24, 24, 64)        256       \n_________________________________________________________________\nconv2d_3 (Conv2D)            (None, 24, 24, 64)        36928     \n_________________________________________________________________\nactivation_3 (Activation)    (None, 24, 24, 64)        0         \n_________________________________________________________________\nbatch_normalization_3 (Batch (None, 24, 24, 64)        256       \n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 (None, 12, 12, 64)        0         \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 12, 12, 64)        0         \n_________________________________________________________________\nconv2d_4 (Conv2D)            (None, 12, 12, 128)       73856     \n_________________________________________________________________\nactivation_4 (Activation)    (None, 12, 12, 128)       0         \n_________________________________________________________________\nbatch_normalization_4 (Batch (None, 12, 12, 128)       512       \n_________________________________________________________________\nconv2d_5 (Conv2D)            (None, 12, 12, 128)       147584    \n_________________________________________________________________\nactivation_5 (Activation)    (None, 12, 12, 128)       0         \n_________________________________________________________________\nbatch_normalization_5 (Batch (None, 12, 12, 128)       512       \n_________________________________________________________________\nmax_pooling2d_2 (MaxPooling2 (None, 6, 6, 128)         0         \n_________________________________________________________________\ndropout_2 (Dropout)          (None, 6, 6, 128)         0         \n_________________________________________________________________\nconv2d_6 (Conv2D)            (None, 6, 6, 256)         295168    \n_________________________________________________________________\nactivation_6 (Activation)    (None, 6, 6, 256)         0         \n_________________________________________________________________\nbatch_normalization_6 (Batch (None, 6, 6, 256)         1024      \n_________________________________________________________________\nconv2d_7 (Conv2D)            (None, 6, 6, 256)         590080    \n_________________________________________________________________\nactivation_7 (Activation)    (None, 6, 6, 256)         0         \n_________________________________________________________________\nbatch_normalization_7 (Batch (None, 6, 6, 256)         1024      \n_________________________________________________________________\nmax_pooling2d_3 (MaxPooling2 (None, 3, 3, 256)         0         \n_________________________________________________________________\ndropout_3 (Dropout)          (None, 3, 3, 256)         0         \n_________________________________________________________________\nflatten (Flatten)            (None, 2304)              0         \n_________________________________________________________________\ndense (Dense)                (None, 64)                147520    \n_________________________________________________________________\nactivation_8 (Activation)    (None, 64)                0         \n_________________________________________________________________\nbatch_normalization_8 (Batch (None, 64)                256       \n_________________________________________________________________\ndropout_4 (Dropout)          (None, 64)                0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 64)                4160      \n_________________________________________________________________\nactivation_9 (Activation)    (None, 64)                0         \n_________________________________________________________________\nbatch_normalization_9 (Batch (None, 64)                256       \n_________________________________________________________________\ndropout_5 (Dropout)          (None, 64)                0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 7)                 455       \n_________________________________________________________________\nactivation_10 (Activation)   (None, 7)                 0         \n=================================================================\nTotal params: 1,328,167\nTrainable params: 1,325,991\nNon-trainable params: 2,176\n_________________________________________________________________\nNone\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Compile and train ","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.optimizers import RMSprop,SGD,Adam\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau","metadata":{"execution":{"iopub.status.busy":"2022-11-28T12:07:57.665592Z","iopub.execute_input":"2022-11-28T12:07:57.666062Z","iopub.status.idle":"2022-11-28T12:07:57.675669Z","shell.execute_reply.started":"2022-11-28T12:07:57.666018Z","shell.execute_reply":"2022-11-28T12:07:57.674350Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"Checkpoint( Function — ModelCheckpoint() )\nIt will monitor the validation loss and will try to minimize the loss using the mode=’min’ property. When the checkpoint is reached it will save the best trained weights. Verbose=1 is just for visualization when the code created checkpoint.Here i am using it’s following parameters:\n\nfile-path: Path to save the model file.Here i am saving the model file with the name EmotionDetectionModel.h5\nmonitor: Quantity to monitor.Here i am monitoring the validation loss.\nmode: One of {auto, min, max}. If save_best_only=True, the decision to overwrite the current save file is made based on either the maximization or the minimization of the monitored quantity.\nsave_best_only: If save_best_only=True, the latest best model according to the quantity monitored will not be overwritten.\nverbose: int. 0: quiet, 1: update messages.\nEarly Stopping ( Function — EarlyStopping() )\nThis will stop the execution early by checking the following properties.\n\nmonitor: Quantity to monitor.Here i am monitoring the validation loss.\nmin_delta: Minimum change in the monitored quantity to qualify as an improvement, i.e. an absolute change of less than min_delta, will count as no improvement.Here i have given it 0.\npatience: Number of epochs with no improvement after which training will be stopped. Here i have given it 3.\nrestore_best_weights: Whether to restore model weights from the epoch with the best value of the monitored quantity. If False, the model weights obtained at the last step of training are used.Here i have given it True.\nverbose: int. 0: quiet, 1: update messages.\nReduce Learning Rate ( Function — ReduceLROnPlateau() )\nModels often benefit from reducing the learning rate by a factor of 2–10 once learning stagnates. This callback monitors a quantity and if no improvement is seen for a ‘patience’ number of epochs, the learning rate is reduced. I have used the following properties for this.\n\nmonitor: To monitor a particular loss. Here i am monitoring the validation loss.\nfactor: Factor by which the learning rate will be reduced. new_lr = lr * factor. Here i am using 0.2 as factor.\npatience: Number of epochs with no improvement after which learning rate will be reduced.Here i am using 3.\nmin_delta: Threshold for measuring the new optimum, to only focus on significant changes.\nverbose: int. 0: quiet, 1: update messages.","metadata":{}},{"cell_type":"code","source":"checkpoint = ModelCheckpoint('EmotionDetectionModel.h5',\n                             monitor='val_loss',\n                             mode='min',\n                             save_best_only=True,\n                             verbose=1)\nearlystop = EarlyStopping(monitor='val_loss',\n                          min_delta=0,\n                          patience=3,\n                          verbose=1,\n                          restore_best_weights=True\n                          )\nreduce_lr = ReduceLROnPlateau(monitor='val_loss',\n                              factor=0.2,\n                              patience=3,\n                              verbose=1,\n                              min_delta=0.0001)\ncallbacks = [earlystop,checkpoint,reduce_lr]","metadata":{"execution":{"iopub.status.busy":"2022-11-28T12:07:57.678654Z","iopub.execute_input":"2022-11-28T12:07:57.679479Z","iopub.status.idle":"2022-11-28T12:07:57.689815Z","shell.execute_reply.started":"2022-11-28T12:07:57.679441Z","shell.execute_reply":"2022-11-28T12:07:57.688960Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"model.compile()\nIt has the following arguments:\n\nloss: This value will determine the type of loss function to use in your code. Here i have categorical data in 5 categories or classes so i have used ‘categorical_crossentropy’ loss.\noptimizer: This value will determine the type of optimizer function to use in your code.Here i have used Adam optimizer with learning rate 0.001 as it is the best optimizer for categorical data.\nmetrics: The metrics argument should be a list — you model can have any number of metrics.It is the list of metrics to be evaluated by the model during training and testing.Here i have used accuracy as metric which will compile mu model according to the accuracy.","metadata":{}},{"cell_type":"markdown","source":"model.fit_generator()\nFits the model on data yielded batch-by-batch by a Python generator.\n\nIt has the following arguments:\n\ngenerator: The train_generator object that we created earlier.\nsteps_per_epochs: The steps to take on the training data in one epoch.\nepochs: The total number of epochs (pass though the whole dataset once).\ncallbacks: The list containing all the callbacks that we created earlier.\nvalidation_data: The validation_generator object that we created earlier.\nvalidation_steps: The steps to take on the validation data in one epoch.\n\n","metadata":{}},{"cell_type":"code","source":"model.compile(loss='categorical_crossentropy',\n optimizer = Adam(lr=0.001),\n metrics=['accuracy'])\nnb_train_samples = 24176\nnb_test_samples = 3006\nepochs=25\nhistory=model.fit_generator(\n train_generator,\n steps_per_epoch=nb_train_samples//batch_size,\n epochs=epochs,\n callbacks=callbacks,\n validation_data=test_generator,\n validation_steps=nb_test_samples//batch_size)","metadata":{"execution":{"iopub.status.busy":"2022-11-28T12:07:57.691272Z","iopub.execute_input":"2022-11-28T12:07:57.692144Z","iopub.status.idle":"2022-11-28T13:53:11.200136Z","shell.execute_reply.started":"2022-11-28T12:07:57.692109Z","shell.execute_reply":"2022-11-28T13:53:11.199293Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n/opt/conda/lib/python3.7/site-packages/keras/engine/training.py:1972: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n  warnings.warn('`Model.fit_generator` is deprecated and '\n2022-11-28 12:07:57.982132: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/25\n755/755 [==============================] - 238s 312ms/step - loss: 2.2179 - accuracy: 0.1898 - val_loss: 1.7830 - val_accuracy: 0.2480\n\nEpoch 00001: val_loss improved from inf to 1.78299, saving model to EmotionDetectionModel.h5\nEpoch 2/25\n755/755 [==============================] - 226s 299ms/step - loss: 1.8416 - accuracy: 0.2296 - val_loss: 1.7873 - val_accuracy: 0.2413\n\nEpoch 00002: val_loss did not improve from 1.78299\nEpoch 3/25\n755/755 [==============================] - 222s 294ms/step - loss: 1.7994 - accuracy: 0.2473 - val_loss: 1.7489 - val_accuracy: 0.2802\n\nEpoch 00003: val_loss improved from 1.78299 to 1.74894, saving model to EmotionDetectionModel.h5\nEpoch 4/25\n755/755 [==============================] - 223s 296ms/step - loss: 1.7898 - accuracy: 0.2533 - val_loss: 1.7410 - val_accuracy: 0.2913\n\nEpoch 00004: val_loss improved from 1.74894 to 1.74099, saving model to EmotionDetectionModel.h5\nEpoch 5/25\n755/755 [==============================] - 224s 296ms/step - loss: 1.7740 - accuracy: 0.2627 - val_loss: 1.7880 - val_accuracy: 0.2722\n\nEpoch 00005: val_loss did not improve from 1.74099\nEpoch 6/25\n755/755 [==============================] - 224s 296ms/step - loss: 1.7255 - accuracy: 0.2948 - val_loss: 1.5814 - val_accuracy: 0.3706\n\nEpoch 00006: val_loss improved from 1.74099 to 1.58141, saving model to EmotionDetectionModel.h5\nEpoch 7/25\n755/755 [==============================] - 224s 296ms/step - loss: 1.6572 - accuracy: 0.3313 - val_loss: 1.5299 - val_accuracy: 0.4197\n\nEpoch 00007: val_loss improved from 1.58141 to 1.52992, saving model to EmotionDetectionModel.h5\nEpoch 8/25\n755/755 [==============================] - 224s 297ms/step - loss: 1.5810 - accuracy: 0.3820 - val_loss: 1.4570 - val_accuracy: 0.4503\n\nEpoch 00008: val_loss improved from 1.52992 to 1.45703, saving model to EmotionDetectionModel.h5\nEpoch 9/25\n755/755 [==============================] - 224s 296ms/step - loss: 1.5216 - accuracy: 0.4103 - val_loss: 1.3817 - val_accuracy: 0.4812\n\nEpoch 00009: val_loss improved from 1.45703 to 1.38166, saving model to EmotionDetectionModel.h5\nEpoch 10/25\n755/755 [==============================] - 225s 298ms/step - loss: 1.4821 - accuracy: 0.4258 - val_loss: 1.2531 - val_accuracy: 0.5323\n\nEpoch 00010: val_loss improved from 1.38166 to 1.25312, saving model to EmotionDetectionModel.h5\nEpoch 11/25\n755/755 [==============================] - 226s 299ms/step - loss: 1.4544 - accuracy: 0.4411 - val_loss: 1.2611 - val_accuracy: 0.4990\n\nEpoch 00011: val_loss did not improve from 1.25312\nEpoch 12/25\n755/755 [==============================] - 223s 296ms/step - loss: 1.4299 - accuracy: 0.4500 - val_loss: 1.1805 - val_accuracy: 0.5380\n\nEpoch 00012: val_loss improved from 1.25312 to 1.18051, saving model to EmotionDetectionModel.h5\nEpoch 13/25\n755/755 [==============================] - 223s 296ms/step - loss: 1.4100 - accuracy: 0.4613 - val_loss: 1.1767 - val_accuracy: 0.5450\n\nEpoch 00013: val_loss improved from 1.18051 to 1.17672, saving model to EmotionDetectionModel.h5\nEpoch 14/25\n755/755 [==============================] - 224s 297ms/step - loss: 1.3956 - accuracy: 0.4689 - val_loss: 1.1648 - val_accuracy: 0.5531\n\nEpoch 00014: val_loss improved from 1.17672 to 1.16480, saving model to EmotionDetectionModel.h5\nEpoch 15/25\n755/755 [==============================] - 224s 297ms/step - loss: 1.3804 - accuracy: 0.4750 - val_loss: 1.1472 - val_accuracy: 0.5669\n\nEpoch 00015: val_loss improved from 1.16480 to 1.14719, saving model to EmotionDetectionModel.h5\nEpoch 16/25\n755/755 [==============================] - 223s 295ms/step - loss: 1.3589 - accuracy: 0.4846 - val_loss: 1.1500 - val_accuracy: 0.5554\n\nEpoch 00016: val_loss did not improve from 1.14719\nEpoch 17/25\n755/755 [==============================] - 223s 295ms/step - loss: 1.3520 - accuracy: 0.4889 - val_loss: 1.1165 - val_accuracy: 0.5773\n\nEpoch 00017: val_loss improved from 1.14719 to 1.11653, saving model to EmotionDetectionModel.h5\nEpoch 18/25\n755/755 [==============================] - 223s 295ms/step - loss: 1.3384 - accuracy: 0.4992 - val_loss: 1.1223 - val_accuracy: 0.5699\n\nEpoch 00018: val_loss did not improve from 1.11653\nEpoch 19/25\n755/755 [==============================] - 224s 296ms/step - loss: 1.3310 - accuracy: 0.4951 - val_loss: 1.1018 - val_accuracy: 0.5766\n\nEpoch 00019: val_loss improved from 1.11653 to 1.10178, saving model to EmotionDetectionModel.h5\nEpoch 20/25\n755/755 [==============================] - 223s 295ms/step - loss: 1.3301 - accuracy: 0.5001 - val_loss: 1.1140 - val_accuracy: 0.5780\n\nEpoch 00020: val_loss did not improve from 1.10178\nEpoch 21/25\n755/755 [==============================] - 220s 292ms/step - loss: 1.3125 - accuracy: 0.5063 - val_loss: 1.0992 - val_accuracy: 0.5860\n\nEpoch 00021: val_loss improved from 1.10178 to 1.09924, saving model to EmotionDetectionModel.h5\nEpoch 22/25\n755/755 [==============================] - 223s 295ms/step - loss: 1.2998 - accuracy: 0.5109 - val_loss: 1.0957 - val_accuracy: 0.5759\n\nEpoch 00022: val_loss improved from 1.09924 to 1.09574, saving model to EmotionDetectionModel.h5\nEpoch 23/25\n755/755 [==============================] - 223s 295ms/step - loss: 1.2988 - accuracy: 0.5151 - val_loss: 1.1191 - val_accuracy: 0.5860\n\nEpoch 00023: val_loss did not improve from 1.09574\nEpoch 24/25\n755/755 [==============================] - 220s 292ms/step - loss: 1.2939 - accuracy: 0.5134 - val_loss: 1.0866 - val_accuracy: 0.5911\n\nEpoch 00024: val_loss improved from 1.09574 to 1.08659, saving model to EmotionDetectionModel.h5\nEpoch 25/25\n755/755 [==============================] - 220s 291ms/step - loss: 1.2796 - accuracy: 0.5226 - val_loss: 1.1191 - val_accuracy: 0.5699\n\nEpoch 00025: val_loss did not improve from 1.08659\n","output_type":"stream"}]}]}